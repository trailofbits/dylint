#![feature(rustc_private)]
#![warn(unused_extern_crates)]

extern crate rustc_errors;
extern crate rustc_hir;
extern crate rustc_span;

use clippy_utils::{attrs::is_doc_hidden, diagnostics::span_lint_and_then, source::snippet_opt};
use litellm_rs::{
    Choice, CompletionOptions, CompletionResponse, Message, MessageContent, MessageRole,
    completion, core::types::responses::FinishReason, user_message,
};
use rustc_hir::{Attribute, FnSig, Item, ItemKind, attrs::AttributeKind};
use rustc_lint::{LateContext, LateLintPass, LintContext};
use rustc_span::{BytePos, SourceFileAndLine, Span};
use serde::Deserialize;
use std::fmt::Write;

const DEFAULT_PROMPT: &str = "An elaborate, high quality rustdoc comment for the above function:";
const DEFAULT_MODEL: &str = "gpt-3.5-turbo";
const DEFAULT_MAX_TOKENS: u32 = 1000;
const DEFAULT_TEMPERATURE: f32 = 0.2;

const MOCK_COMPLETION: &str = "/// A doc comment generated by an LLM.\n";

const STOP: &str = "\n```";

dylint_linting::impl_late_lint! {
    /// ⚠️ DO NOT RUN THIS LINT ON PRIVATE SOURCE CODE ⚠️
    ///
    /// ### What it does
    ///
    /// Checks for functions missing [doc comments].
    ///
    /// ### Why is this bad?
    ///
    /// Understanding what a function does is easier given a description of the function rather than
    /// just its code.
    ///
    /// ### Known problems
    ///
    /// The lint is currently enabled only for functions.
    ///
    /// ### Example
    ///
    /// ```rust
    /// pub fn foo() {}
    /// ```
    ///
    /// Use instead:
    ///
    /// ```rust
    /// /// A doc comment generated by an LLM.
    /// pub fn foo() {}
    /// ```
    ///
    /// ### LLM Integration
    ///
    /// The lint will suggest a doc comment generated by an LLM via the [litellm-rs] library.
    /// The appropriate API key environment variable must be set based on the model being used
    /// (e.g., `ANTHROPIC_API_KEY` for Anthropic models, `OPENAI_API_KEY` for OpenAI models).
    /// The prompt sent to the LLM has the following form:
    ///
    /// ````ignore
    /// ```rust
    /// <function declaration>
    /// ```
    /// An elaborate, high quality rustdoc comment for the above function:
    /// ```rust
    /// ````
    ///
    /// The prompt's [`stop` parameter] is set to `["\n```"]`. Thus, the LLM should stop generating
    /// tokens once the second code block is complete. The suggested doc comment is the one that
    /// appears in that code block, if any.
    ///
    /// The phrase "An elaborate..." is configurable (see below).
    ///
    /// ### Configuration
    ///
    /// Certain [LLM parameters] can be configured by setting them in the
    /// `missing_doc_comment_llm` table of the linted workspace's [`dylint.toml` file]. Example:
    ///
    /// ```toml
    /// [missing_doc_comment_llm]
    /// prompt = "A rustdoc comment for the above function with a \"Motivation\" section:"
    /// temperature = 1.0
    /// ```
    ///
    /// The following parameters are supported:
    ///
    /// - `prompt` (default "An elaborate, high quality rustdoc comment for the above function:").
    ///   This default is based on the [Write a Python docstring] example.
    /// - `model` (default "gpt-3.5-turbo")
    /// - `temperature` (default 0.2). Note that this default is less than the typical default (1.0).
    ///   Per the [`temperature` documentation], "Higher values like 0.8 will make the output more
    ///   random, while lower values like 0.2 will make it more focused and deterministic."
    /// - `top_p` (default none, i.e., use provider's default)
    /// - `presence_penalty` (default none, i.e., use provider's default)
    /// - `frequency_penalty` (default none, i.e., use provider's default)
    ///
    /// [LLM parameters]: https://platform.openai.com/docs/api-reference/chat/create
    /// [Write a Python docstring]: https://platform.openai.com/examples/default-python-docstring
    /// [`dylint.toml` file]: https://github.com/trailofbits/dylint#configurable-libraries
    /// [`stop` parameter]: https://platform.openai.com/docs/api-reference/chat/create#chat-create-stop
    /// [`temperature` documentation]: https://platform.openai.com/docs/api-reference/chat/create#chat-create-temperature
    /// [doc comments]: https://doc.rust-lang.org/rust-by-example/meta/doc.html#doc-comments
    /// [litellm-rs]: https://docs.rs/litellm-rs/
    pub MISSING_DOC_COMMENT_LLM,
    Warn,
    "description goes here",
    MissingDocCommentLlm::new()
}

#[derive(Default, Deserialize)]
struct Config {
    prompt: Option<String>,
    model: Option<String>,
    max_tokens: Option<u32>,
    temperature: Option<f32>,
    top_p: Option<f32>,
    frequency_penalty: Option<f32>,
    presence_penalty: Option<f32>,
}

struct MissingDocCommentLlm {
    config: Config,
}

impl MissingDocCommentLlm {
    pub fn new() -> Self {
        Self {
            config: dylint_linting::config_or_default(env!("CARGO_PKG_NAME")),
        }
    }
}

impl<'tcx> LateLintPass<'tcx> for MissingDocCommentLlm {
    fn check_item(&mut self, cx: &LateContext<'tcx>, item: &'tcx Item<'tcx>) {
        let owner_id = item.owner_id;

        // smoelius: The next two checks were copied from:
        // https://github.com/rust-lang/rust-clippy/blob/92c4f1e2d9db43ebc0449fbbc2150eeb9429e65b/clippy_lints/src/doc.rs#L372-L384

        if !cx.effective_visibilities.is_exported(owner_id.def_id) {
            return; // Private functions do not require doc comments
        }

        // Do not lint if any parent has `#[doc(hidden)]` attribute (#7347)
        if cx
            .tcx
            .hir_parent_iter(owner_id.into())
            .any(|(id, _node)| is_doc_hidden(cx.tcx.hir_attrs(id)))
        {
            return;
        }

        // smoelius: Only enable for functions for now.
        let ItemKind::Fn {
            sig: FnSig {
                span: fn_sig_span, ..
            },
            ..
        } = item.kind
        else {
            return;
        };

        if cx
            .tcx
            .hir_attrs(item.hir_id())
            .iter()
            .any(|attr| matches!(attr, Attribute::Parsed(AttributeKind::DocComment { .. })))
        {
            return;
        }

        let doc_comment = (|| {
            let snippet = snippet_opt(cx, item.span)?;
            let prompt = self.prompt_from_snippet(&snippet);
            let (model, options) = self.completion_options_from_config();

            let rt = match tokio::runtime::Runtime::new() {
                Ok(rt) => rt,
                Err(error) => {
                    cx.sess().dcx().span_warn(fn_sig_span, error.to_string());
                    return None;
                }
            };

            let response =
                match rt.block_on(async { send_request(&model, &prompt, &options).await }) {
                    Ok(response) => response,
                    Err(error) => {
                        cx.sess().dcx().span_warn(fn_sig_span, error.to_string());
                        return None;
                    }
                };

            response
                .choices
                .first()
                .and_then(|choice| {
                    let text = choice
                        .message
                        .content
                        .as_ref()
                        .map(|content| content.to_string())?;
                    extract_doc_comment(&text)
                })
                .or_else(|| {
                    cx.sess().dcx().span_warn(
                        fn_sig_span,
                        format!("Could not extract doc comment from response: {response:#?}",),
                    );
                    None
                })
        })();

        let insertion_point = skip_preceding_line_comments(cx, earliest_attr_span(cx, item));

        span_lint_and_then(
            cx,
            MISSING_DOC_COMMENT_LLM,
            fn_sig_span,
            "exported function lacks a doc comment",
            |diag| {
                if let Some(doc_comment) = doc_comment {
                    diag.span_suggestion(
                        insertion_point.with_hi(insertion_point.lo()),
                        "use the following suggestion from the LLM",
                        doc_comment,
                        rustc_errors::Applicability::MachineApplicable,
                    );
                }
            },
        );
    }
}

impl MissingDocCommentLlm {
    fn completion_options_from_config(&self) -> (String, CompletionOptions) {
        let model = self
            .config
            .model
            .as_deref()
            .unwrap_or(DEFAULT_MODEL)
            .to_owned();

        let options = CompletionOptions {
            max_tokens: Some(self.config.max_tokens.unwrap_or(DEFAULT_MAX_TOKENS)),
            temperature: Some(self.config.temperature.unwrap_or(DEFAULT_TEMPERATURE)),
            top_p: self.config.top_p,
            frequency_penalty: self.config.frequency_penalty,
            presence_penalty: self.config.presence_penalty,
            stop: Some(vec![STOP.to_owned()]),
            ..Default::default()
        };

        (model, options)
    }

    fn prompt_from_snippet(&self, snippet: &str) -> String {
        format!(
            "```rust\n{snippet}\n```\n{}\n```rust\n",
            self.config.prompt.as_deref().unwrap_or(DEFAULT_PROMPT)
        )
    }
}

async fn send_request(
    model: &str,
    prompt: &str,
    options: &CompletionOptions,
) -> Result<CompletionResponse, Box<dyn std::error::Error>> {
    if testing() {
        // Return a mock response for testing
        return Ok(CompletionResponse {
            id: "test".to_owned(),
            object: "chat.completion".to_owned(),
            created: 0,
            model: model.to_owned(),
            choices: vec![Choice {
                index: 0,
                message: Message {
                    role: MessageRole::Assistant,
                    content: Some(MessageContent::Text(MOCK_COMPLETION.to_owned())),
                    name: None,
                    tool_calls: None,
                    tool_call_id: None,
                    function_call: None,
                },
                finish_reason: Some(FinishReason::Stop),
            }],
            usage: None,
        });
    }

    completion(model, vec![user_message(prompt)], Some(options.clone()))
        .await
        .map_err(|e| Box::new(e) as Box<dyn std::error::Error>)
}

fn extract_doc_comment(response: &str) -> Option<String> {
    // smoelius: Sanity check. The LLM response should not contain the stop sequence.
    assert_ne!(response.lines().last(), Some(STOP));

    // smoelius: In several of my experiments, the last several lines of the response did not start
    // with `///`. Ignore those lines. Also, in some of my experiments, the generated comments
    // were internal attributes, i.e., started with `//!`. Convert those to external attributes.
    let mut comment = String::new();
    let mut found_doc_comment = false;

    for line in response.lines() {
        // Start collecting when we find a doc comment line
        if line.starts_with("//!") || line.starts_with("///") {
            found_doc_comment = true;
            if let Some(s) = line.strip_prefix("//!") {
                writeln!(&mut comment, "///{s}").unwrap();
            } else {
                writeln!(&mut comment, "{line}").unwrap();
            }
        } else if found_doc_comment {
            // Stop when we encounter a non-doc-comment line after finding doc comments
            break;
        }
    }

    if comment.is_empty() {
        None
    } else {
        Some(comment)
    }
}

fn earliest_attr_span(cx: &LateContext<'_>, item: &Item<'_>) -> Span {
    cx.tcx
        .hir_attrs(item.hir_id())
        .iter()
        .filter_map(|attr| match attr {
            Attribute::Unparsed(_) => Some(attr.span()),
            Attribute::Parsed(_) => None,
        })
        .fold(
            item.span,
            |lhs, rhs| {
                if lhs.lo() <= rhs.lo() { lhs } else { rhs }
            },
        )
}

fn skip_preceding_line_comments(cx: &LateContext<'_>, mut span: Span) -> Span {
    while span.lo() >= BytePos(1) {
        let SourceFileAndLine { sf, line } = cx
            .sess()
            .source_map()
            .lookup_line(span.lo() - BytePos(1))
            .unwrap();
        let lo_prev_relative = sf.lines()[line];
        let lo_prev = sf.absolute_position(lo_prev_relative);
        let span_prev = span.with_lo(lo_prev);
        if snippet_opt(cx, span_prev).is_some_and(|snippet| snippet.starts_with("//")) {
            span = span_prev;
        } else {
            break;
        }
    }
    span
}

#[must_use]
fn testing() -> bool {
    enabled("TESTING")
}

#[must_use]
fn enabled(name: &str) -> bool {
    let key = env!("CARGO_PKG_NAME").to_uppercase() + "_" + name;
    std::env::var(key).is_ok_and(|value| value != "0")
}

#[cfg(test)]
mod test {
    use super::*;
    use std::env::set_var;

    const MISSING_DOC_COMMENT_LLM_TESTING: &str = "MISSING_DOC_COMMENT_LLM_TESTING";

    #[test]
    fn ui() {
        unsafe {
            set_var(MISSING_DOC_COMMENT_LLM_TESTING, "1");
        }

        let toml = format!(
            r#"
[missing_doc_comment_llm]
prompt = "{DEFAULT_PROMPT}"
meld = "{DEFAULT_MODEL}"
max_tokens = {DEFAULT_MAX_TOKENS}
temperature = {DEFAULT_TEMPERATURE}
top_p = 1.0
frequency_penalty = 0.0
presence_penalty = 0.0
"#
        );

        dylint_testing::ui::Test::src_base(env!("CARGO_PKG_NAME"), "ui")
            .dylint_toml(toml)
            .run();
    }
}
